{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot to show test error as a function of data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T23:22:46.597378Z",
     "start_time": "2018-06-21T23:22:27.347934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import calendar\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as tkr\n",
    "import seaborn as sns\n",
    "import dill as pickle\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn import metrics\n",
    "from pymatgen.matproj.rest import MPRester\n",
    "from gaspy_regress.preprocessor import GASpyPreprocessor\n",
    "from gaspy import defaults, gasdb, utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams.update({'font.sans-serif': 'Helvetica'})\n",
    "rcParams.update({'font.family': 'sans-serif'})\n",
    "rcParams.update({'font.size': 7})\n",
    "rcParams.update({'savefig.dpi': 600})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T23:26:06.968868Z",
     "start_time": "2018-06-21T23:26:06.962651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Regression settings\n",
    "features = ['coordatoms_chemfp0', 'neighbors_chemfp0']\n",
    "dim_red = 'pca'\n",
    "responses = ['energy']\n",
    "blocks = ['adsorbate']\n",
    "fit_blocks = [('CO',), ('H',)]\n",
    "adsorbates = ['CO', 'H']\n",
    "# Pull data only with these VASP settings\n",
    "VASP_SETTINGS = utils.vasp_settings_to_str({'gga': 'RP',\n",
    "                                            'pp_version': '5.4',\n",
    "                                            'encut': 350})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parse our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T23:25:31.344131Z",
     "start_time": "2018-06-21T23:22:46.605345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pull all of the documents and sort them by date\n",
    "fingerprints = defaults.fingerprints(simulated=True)\n",
    "fingerprints['symbols'] = '$atoms.chemical_symbols'  # needed for preprocessor\n",
    "docs = gasdb.get_docs(fingerprints=fingerprints, **defaults.doc_filters())\n",
    "docs = sorted(docs, key=lambda doc: doc['adslab_calculation_date'])\n",
    "\n",
    "# Initialize a dictionary whose keys are the mpids we've studied\n",
    "# and whose values are their compositions. This can be used to filter\n",
    "# the documents\n",
    "mpids = set(doc['mpid'] for doc in docs)\n",
    "compositions_by_mpid = dict.fromkeys(mpids)\n",
    "# Open up MP connection\n",
    "mp_key = utils.read_rc('matproj_api_key')\n",
    "with MPRester(mp_key) as mp_db:\n",
    "    # Populate that dictionary, `compositions_by_mpid`\n",
    "    for mpid in tqdm.tqdm(mpids):\n",
    "        entry = mp_db.get_entry_by_material_id({'task_ids': mpid})\n",
    "        comp = set(entry.as_dict()['composition'].keys())\n",
    "        compositions_by_mpid[mpid] = set(comp)\n",
    "\n",
    "# Define the elements to exclude from our study\n",
    "excluded_elements = set(['Ca', 'Na', 'Nb', 'S', 'Se'])\n",
    "# Use the `compositions_by_mpid` dictionary to identify the mpid numbers to exclude.\n",
    "mpids_to_exclude = []\n",
    "for mpid, comp in compositions_by_mpid.iteritems():\n",
    "    if excluded_elements.intersection(comp):\n",
    "        mpids_to_exclude.append(mpid)\n",
    "mpids_to_exclude = set(mpids_to_exclude)\n",
    "\n",
    "# Finally, reconstruct docs to exclude the elements\n",
    "docs = [doc for doc in docs if doc['mpid'] not in mpids_to_exclude]\n",
    "# Add a key to the docs that we're probably going to need\n",
    "for i, doc in enumerate(docs):\n",
    "    ads = doc['adsorbates'][0]\n",
    "    docs[i]['adsorbate'] = ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T23:25:33.687264Z",
     "start_time": "2018-06-21T23:25:31.346302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped the cell's code and loaded variables dfs from file '/global/project/projectdirs/m2755/manuscripts/GASpy/figures/hits/errors.pkl'.]\n",
      "For CO and given a training set with 200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=83, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 600 points, we get:\n",
      "[('stackingestimator-1', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.85, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))), ('stackingestimator-2', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.99, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='lad', max_depth=9, max_features=0.3,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=8,\n",
      "             min_samples_split=15, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.5, verbose=0, warm_start=False))), ('linearsvr', LinearSVR(C=0.1, dual=True, epsilon=0.001, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=None, tol=0.0001, verbose=0))]\n",
      "\n",
      "For CO and given a training set with 800 points, we get:\n",
      "[('onehotencoder', OneHotEncoder(categorical_features=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True],\n",
      "       dtype=<type 'float'>, minimum_fraction=0.25, sparse=False,\n",
      "       threshold=10)), ('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=13, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.5))]\n",
      "\n",
      "For CO and given a training set with 1000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))), ('decisiontreeregressor', DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=17,\n",
      "           min_samples_split=17, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best'))]\n",
      "\n",
      "For CO and given a training set with 1200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.45, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=18,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 1400 points, we get:\n",
      "[('selectfwe', SelectFwe(alpha=0.021, score_func=<function f_regression at 0x2b7119ed48c0>)), ('gradientboostingregressor', GradientBoostingRegressor(alpha=0.75, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='lad', max_depth=3, max_features=0.85,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=16,\n",
      "             min_samples_split=13, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.35, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 1600 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=2, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.25))]\n",
      "\n",
      "For CO and given a training set with 1800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.001, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=4, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.25))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.75, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=12, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 2000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.15, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.4, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=12,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 2200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 2400 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.75, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=4, min_samples_split=19,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 2600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.45, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=10, min_samples_split=12,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 2800 points, we get:\n",
      "[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingregressor', GradientBoostingRegressor(alpha=0.85, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='lad', max_depth=8, max_features=0.1,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=8,\n",
      "             min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.3, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 3000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.25, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.1, verbose=0))), ('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=12, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1.0))]\n",
      "\n",
      "For CO and given a training set with 3200 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.95, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=2, min_samples_split=9,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))), ('gradientboostingregressor', GradientBoostingRegressor(alpha=0.99, criterion='friedman_mse', init=None,\n",
      "             learning_rate=1.0, loss='lad', max_depth=6, max_features=0.6,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=15,\n",
      "             min_samples_split=16, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.9, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 3400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=5, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 3600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.65, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=7, min_samples_split=8,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=6,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 3800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.1, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=15,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.4, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.001, verbose=0))]\n",
      "\n",
      "For CO and given a training set with 4000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.25, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=12,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('selectfwe', SelectFwe(alpha=0.037, score_func=<function f_regression at 0x2b7119ed48c0>)), ('gradientboostingregressor', GradientBoostingRegressor(alpha=0.85, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='huber', max_depth=8,\n",
      "             max_features=0.4, max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=3, min_samples_split=18,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=0.55, verbose=0,\n",
      "             warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 4200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=11, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 4400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 4600 points, we get:\n",
      "[('normalizer', Normalizer(copy=True, norm='l2')), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=9, min_samples_split=17,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 4800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.3, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=4, min_samples_split=11,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=29, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 5000 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=13,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 5200 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=9, max_features=0.15,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=11,\n",
      "             min_samples_split=15, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.35, verbose=0, warm_start=False))), ('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=9, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.65))]\n",
      "\n",
      "For CO and given a training set with 5400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=44, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 5600 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=79, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 5800 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=16, min_samples_split=14,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 6000 points, we get:\n",
      "[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.35, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=9,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 6200 points, we get:\n",
      "[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=4, min_samples_split=5,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 6400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=24, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 6600 points, we get:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=50, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 6800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=51, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 7000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=22, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 7200 points, we get:\n",
      "[('gradientboostingregressor', GradientBoostingRegressor(alpha=0.75, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=10, max_features=0.95,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=8,\n",
      "             min_samples_split=18, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.7, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 7400 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=13, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8))]\n",
      "\n",
      "For CO and given a training set with 7600 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 7800 points, we get:\n",
      "[('stackingestimator-1', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=18, p=2,\n",
      "          weights='distance'))), ('stackingestimator-2', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.05, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=12, min_samples_split=9,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=4,\n",
      "          min_samples_split=13, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 8000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 8200 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=16, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1.0))]\n",
      "\n",
      "For CO and given a training set with 8400 points, we get:\n",
      "[('featureunion', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True)), ('functiontransformer-2', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True))],\n",
      "       transformer_weights=None)), ('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.15, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=19,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=15, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 8600 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=25, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 8800 points, we get:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=2, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 9000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.2, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.01, verbose=0))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=22, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 9200 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.1, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.1, verbose=0))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=4, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 9400 points, we get:\n",
      "[('fastica', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
      "    n_components=None, random_state=None, tol=0.25, w_init=None,\n",
      "    whiten=True)), ('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=9, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9))]\n",
      "\n",
      "For CO and given a training set with 9600 points, we get:\n",
      "[('fastica', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
      "    n_components=None, random_state=None, tol=0.75, w_init=None,\n",
      "    whiten=True)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "          weights='uniform'))]\n",
      "\n",
      "For CO and given a training set with 9800 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.35, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 10000 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.65, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=9,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 10200 points, we get:\n",
      "[('featureunion', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True)), ('functiontransformer-2', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True))],\n",
      "       transformer_weights=None)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=12, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 10400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 10600 points, we get:\n",
      "[('variancethreshold', VarianceThreshold(threshold=0.6)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=36, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 10800 points, we get:\n",
      "[('selectfwe', SelectFwe(alpha=0.015, score_func=<function f_regression at 0x2b7119ed48c0>)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.35, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 11000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=15, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.55))), ('ridgecv', RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))]\n",
      "\n",
      "For CO and given a training set with 11200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 11400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.4, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.6, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=1e-05, verbose=0))]\n",
      "\n",
      "For CO and given a training set with 11600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=1e-05, verbose=0))), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.05, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=7, min_samples_split=17,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 11800 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=13, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.35))]\n",
      "\n",
      "For CO and given a training set with 12000 points, we get:\n",
      "[('zerocount', ZeroCount()), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=18, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 12200 points, we get:\n",
      "[('selectpercentile', SelectPercentile(percentile=20,\n",
      "         score_func=<function f_regression at 0x2b7119ed48c0>)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 12400 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=17, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8))]\n",
      "\n",
      "For CO and given a training set with 12600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=1.0, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.1, verbose=0))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.9, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=5, min_samples_split=6,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 12800 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=11,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 13000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=24, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 13200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=7,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 13400 points, we get:\n",
      "[('gradientboostingregressor', GradientBoostingRegressor(alpha=0.75, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=9, max_features=0.7,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=4,\n",
      "             min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.9, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 13600 points, we get:\n",
      "[('stackingestimator-1', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=1.0, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=3, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.55))), ('stackingestimator-2', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=9, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=2, p=1,\n",
      "          weights='uniform'))]\n",
      "\n",
      "For CO and given a training set with 13800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=23, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 14000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=9, p=1,\n",
      "          weights='distance'))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.65, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=19, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 14200 points, we get:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.15, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 14400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=73, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 14600 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=11, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 14800 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.3, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=9, min_samples_split=16,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 15000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.1, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=25, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 15200 points, we get:\n",
      "[('maxabsscaler', MaxAbsScaler(copy=True)), ('extratreesregressor', ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.85, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 15400 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.6, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 15600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.6, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For CO and given a training set with 15800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=24, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 16000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=24, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 16200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=51, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For CO and given a training set with 16258 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=5, min_samples_split=20,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=3, min_samples_split=9,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 200 points, we get:\n",
      "[('ridgecv', RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))]\n",
      "\n",
      "For H and given a training set with 400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.8, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.001, loss='lad', max_depth=10,\n",
      "             max_features=0.6, max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=3, min_samples_split=13,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=0.9, verbose=0,\n",
      "             warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.7, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))]\n",
      "\n",
      "For H and given a training set with 600 points, we get:\n",
      "[('gradientboostingregressor', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='lad', max_depth=10, max_features=0.4,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=3,\n",
      "             min_samples_split=14, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.7, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=19, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.85))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.2, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=11,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 1000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.75, criterion='friedman_mse', init=None,\n",
      "             learning_rate=1.0, loss='lad', max_depth=8, max_features=0.4,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=7,\n",
      "             min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.85, verbose=0, warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.7, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.1, verbose=0))]\n",
      "\n",
      "For H and given a training set with 1200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=12,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 1400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.2, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=3,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('decisiontreeregressor', DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=10,\n",
      "           min_samples_split=18, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best'))]\n",
      "\n",
      "For H and given a training set with 1600 points, we get:\n",
      "[('maxabsscaler', MaxAbsScaler(copy=True)), ('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.15, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.1, verbose=0))), ('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.4, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=10,\n",
      "          min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 1800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=99, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 2000 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=18,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 2200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.15, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 2400 points, we get:\n",
      "[('gradientboostingregressor', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='huber', max_depth=7,\n",
      "             max_features=0.85, max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=17, min_samples_split=7,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=0.75, verbose=0,\n",
      "             warm_start=False))]\n",
      "\n",
      "For H and given a training set with 2600 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.95, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=6, min_samples_split=4,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 2800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 3000 points, we get:\n",
      "[('fastica', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
      "    n_components=None, random_state=None, tol=0.95, w_init=None,\n",
      "    whiten=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.55, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=8,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 3200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=6, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 3400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=19,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.25, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.001, verbose=0))]\n",
      "\n",
      "For H and given a training set with 3600 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.95, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 3800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.75, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.5, loss='lad', max_depth=6, max_features=0.9,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=11,\n",
      "             min_samples_split=9, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.55, verbose=0, warm_start=False))), ('onehotencoder', OneHotEncoder(categorical_features=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
      "       dtype=<type 'float'>, minimum_fraction=0.15, sparse=False,\n",
      "       threshold=10)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 4000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=21, p=2,\n",
      "          weights='uniform'))]\n",
      "\n",
      "For H and given a training set with 4200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=11, min_samples_split=5,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 4400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 4600 points, we get:\n",
      "[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.85, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=5, min_samples_split=5,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 4800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=39, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 5000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.75, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=6,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 5200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=25, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 5400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=7, p=1,\n",
      "          weights='distance'))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.95, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=1e-05, verbose=0))]\n",
      "\n",
      "For H and given a training set with 5600 points, we get:\n",
      "[('gradientboostingregressor', GradientBoostingRegressor(alpha=0.8, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='huber', max_depth=8,\n",
      "             max_features=0.35, max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=2, min_samples_split=7,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False))]\n",
      "\n",
      "For H and given a training set with 5800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=3, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 6000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 6200 points, we get:\n",
      "[('xgbregressor', XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.5, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=15, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.55))]\n",
      "\n",
      "For H and given a training set with 6400 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.85, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=5, min_samples_split=11,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 6600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.001, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=9, missing=nan, n_estimators=100, nthread=1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.1))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 6800 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.95, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=3, min_samples_split=7,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 7000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=12, min_samples_split=20,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=90, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 7200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=8, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 7400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=35, p=2,\n",
      "          weights='distance'))), ('decisiontreeregressor', DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=13,\n",
      "           min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best'))]\n",
      "\n",
      "For H and given a training set with 7600 points, we get:\n",
      "[('selectfwe', SelectFwe(alpha=0.04, score_func=<function f_regression at 0x2b7119ed48c0>)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 7800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=47, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 8000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 8200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=31, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 8400 points, we get:\n",
      "[('stackingestimator-1', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=44, p=2,\n",
      "          weights='distance'))), ('stackingestimator-2', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=24, p=2,\n",
      "          weights='uniform'))), ('lassolarscv', LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=False, positive=False, precompute='auto', verbose=False))]\n",
      "\n",
      "For H and given a training set with 8600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=1.0, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=11, min_samples_split=18,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=2, p=1,\n",
      "          weights='uniform'))]\n",
      "\n",
      "For H and given a training set with 8800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=26, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 9000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=11, p=1,\n",
      "          weights='distance'))), ('lassolarscv', LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))]\n",
      "\n",
      "For H and given a training set with 9200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=4, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 9400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.15, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 9600 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=3, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 9800 points, we get:\n",
      "[('maxabsscaler', MaxAbsScaler(copy=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=9,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 10000 points, we get:\n",
      "[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.35, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=4, min_samples_split=7,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 10200 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.7, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=3, min_samples_split=7,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 10400 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 10600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.75, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 10800 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.55, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=7,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 11000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=LinearSVR(C=25.0, dual=False, epsilon=0.001, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
      "     max_iter=1000, random_state=None, tol=0.001, verbose=0))), ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.6, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=6, min_samples_split=12,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 11200 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.6, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=7, min_samples_split=19,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.65, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=15, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 11400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=1,\n",
      "          weights='distance'))), ('linearsvr', LinearSVR(C=0.1, dual=True, epsilon=0.01, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=None, tol=0.01, verbose=0))]\n",
      "\n",
      "For H and given a training set with 11600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=11,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.2, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=19, min_samples_split=19,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 11800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=20, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 12000 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.7, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=7,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 12200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=69, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 12400 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.35, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=5,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 12600 points, we get:\n",
      "[('featureunion', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True)), ('functiontransformer-2', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True))],\n",
      "       transformer_weights=None)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 12800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.5, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=6,\n",
      "          min_samples_split=14, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=7,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 13000 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.45, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=13, min_samples_split=16,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 13200 points, we get:\n",
      "[('zerocount', ZeroCount()), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=59, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 13400 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.7, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=1,\n",
      "          min_samples_split=15, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 13600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.35, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=11, min_samples_split=20,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 13800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=22, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 14000 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=33, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 14200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.75, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=9, min_samples_split=13,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 14400 points, we get:\n",
      "[('onehotencoder', OneHotEncoder(categorical_features=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],\n",
      "       dtype=<type 'float'>, minimum_fraction=0.15, sparse=False,\n",
      "       threshold=10)), ('gradientboostingregressor', GradientBoostingRegressor(alpha=0.85, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='lad', max_depth=6, max_features=0.15,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=8,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=0.8, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 14600 points, we get:\n",
      "[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 14800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=16, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 15000 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.4, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=10, min_samples_split=3,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 15200 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=38, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 15400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=6,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('ridgecv', RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))]\n",
      "\n",
      "For H and given a training set with 15600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.7, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=5, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 15800 points, we get:\n",
      "[('maxabsscaler', MaxAbsScaler(copy=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=15,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 16000 points, we get:\n",
      "[('normalizer', Normalizer(copy=True, norm='l1')), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 16200 points, we get:\n",
      "[('selectfwe', SelectFwe(alpha=0.01, score_func=<function f_regression at 0x2b7119ed48c0>)), ('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=50, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 16400 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=36, p=1,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 16600 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.9, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=12, min_samples_split=7,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 16800 points, we get:\n",
      "[('kneighborsregressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=27, p=2,\n",
      "          weights='distance'))]\n",
      "\n",
      "For H and given a training set with 17000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.2, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=15,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('lassolarscv', LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))]\n",
      "\n",
      "For H and given a training set with 17200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.6, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=16,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 17400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.8, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=5, min_samples_split=18,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 17600 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.15, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=4,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 17800 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.95, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='huber', max_depth=10,\n",
      "             max_features=1.0, max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=17,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=0.7, verbose=0,\n",
      "             warm_start=False))), ('ridgecv', RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))]\n",
      "\n",
      "For H and given a training set with 18000 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=50, p=2,\n",
      "          weights='distance'))), ('maxabsscaler', MaxAbsScaler(copy=True)), ('ridgecv', RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False))]\n",
      "\n",
      "For H and given a training set with 18200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.25, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=7, min_samples_split=13,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 18400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.45, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('elasticnetcv', ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=0.4, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "       normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))]\n",
      "\n",
      "For H and given a training set with 18600 points, we get:\n",
      "[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features=0.55, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=7,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 18800 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.4, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=12, min_samples_split=17,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 19000 points, we get:\n",
      "[('featureunion', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True)), ('functiontransformer-2', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function copy at 0x2b70d5e1acf8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=True))],\n",
      "       transformer_weights=None)), ('extratreesregressor', ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.75, max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=6,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 19200 points, we get:\n",
      "[('randomforestregressor', RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=13, min_samples_split=20,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]\n",
      "\n",
      "For H and given a training set with 19400 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features=0.8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=2, min_samples_split=5,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False))), ('lassolarscv', LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=True, positive=False, precompute='auto', verbose=False))]\n",
      "\n",
      "For H and given a training set with 19494 points, we get:\n",
      "[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "           max_features=0.15, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=8, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))), ('lassolarscv', LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
      "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
      "      normalize=False, positive=False, precompute='auto', verbose=False))]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "  9%|         | 143/1523 [00:30<04:49,  4.76it/s]"
     ]
    }
   ],
   "source": [
    "%%cache errors.pkl dfs\n",
    "\n",
    "# Decide how big each test set should be\n",
    "test_size = 200\n",
    "\n",
    "# Pull out the correct adsorbate data and initialize the output dataframes\n",
    "dfs = dict.fromkeys(adsorbates)\n",
    "for ads in adsorbates:\n",
    "    ads_docs = [doc for doc in docs if ads in doc['adsorbates']]\n",
    "    # We'll eventually be storing the data in a Pandas dataframe.\n",
    "    # Here we initialize the dictionary that'll be used to construct the dataframe\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    # Get the correct data\n",
    "    n_splits = len(ads_docs) // test_size\n",
    "    #for split_number in reversed(range(1, n_splits+1)):\n",
    "    for split_number in range(1, n_splits+1):\n",
    "        split_docs = ads_docs[:test_size*(split_number+1)]\n",
    "        training_size = len(split_docs) - test_size\n",
    "\n",
    "        # If we've already done this point, just open it\n",
    "        try:\n",
    "            with open('models/%s_%i.pkl' % (ads, training_size), 'rb') as f_handle:\n",
    "                pp, tpot, train_errors, test_errors = pickle.load(f_handle)\n",
    "\n",
    "        # Otherwise fit the regressor and save it\n",
    "        except IOError:\n",
    "            # Initialize the preprocessing pipeline\n",
    "            pp = GASpyPreprocessor(docs=split_docs[:-test_size],\n",
    "                                   features=features, dim_red=dim_red)\n",
    "            tpot = TPOTRegressor(\n",
    "                                 generations=1,\n",
    "                                 population_size=16,\n",
    "                                 offspring_size=16,\n",
    "                                 verbosity=2,\n",
    "                                 #random_state=42,\n",
    "                                 scoring='neg_median_absolute_error',\n",
    "                                 #scoring='neg_mean_absolute_error',\n",
    "                                 #scoring='neg_mean_squared_error',\n",
    "                                 n_jobs=16,\n",
    "                                )\n",
    "            # Establish the training data and then train\n",
    "            x = pp.transform(split_docs)\n",
    "            y = np.array([doc['energy'] for doc in split_docs])\n",
    "            x_train = x[:-test_size]\n",
    "            y_train = y[:-test_size]\n",
    "            tpot.fit(x_train, y_train)\n",
    "            # Make predictions then calculate errors\n",
    "            y_pred = tpot.predict(x)\n",
    "            y_test = y[-test_size:]\n",
    "            train_errors = y_pred[:-test_size] - y_train\n",
    "            test_errors = y_pred[-test_size:] - y_test\n",
    "            # Pickle the preprocessor, model, and errors\n",
    "            tpot = tpot.fitted_pipeline_\n",
    "            with open('models/%s_%i.pkl' % (ads, training_size), 'wb') as f_handle:\n",
    "                pickle.dump((pp, tpot, train_errors, test_errors), f_handle)\n",
    "\n",
    "        # Report the results\n",
    "        print('For %s and given a training set with %i points, we get:\\n%s\\n'\n",
    "              % (ads, training_size, tpot.steps))\n",
    "        # Store the test documents along with their errors\n",
    "        for doc in split_docs[-test_size:]:\n",
    "            for key, value in doc.iteritems():\n",
    "                data[key].append(value)\n",
    "        data['Error [eV]'].extend(test_errors)\n",
    "    # Create the dataframe\n",
    "    dfs[ads] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting convergence of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data\n",
    "We'll eventually be plotting the number of hits. We know if a site is a \"hit\" based on whether or not it shows up as the representative site in the databall of surfaces. Let's import that databall and then find all of the pertinent mongo IDs of hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T04:43:14.769860Z",
     "start_time": "2018-06-22T04:42:57.852681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set ranges\n",
    "hit_ranges = dict(CO=(-0.77, -0.57), H=(-0.37, -0.17))\n",
    "\n",
    "# Pull the databalls\n",
    "hits_mongo_ids = []\n",
    "for file_name in ['../CO2RR_predictions.pkl', '../HER_predictions.pkl']:\n",
    "    with open(file_name, 'rb') as file_handle:\n",
    "        dft_results, ml_results = pickle.load(file_handle)\n",
    "    # Sift through all of the DFT data and find surfaces\n",
    "    # that are within the ideal bounds\n",
    "    for doc, _, _ in dft_results:\n",
    "        lower_bound, upper_bound = hit_ranges[doc['adsorbates'][0]]\n",
    "        if lower_bound < doc['energy'] < upper_bound:\n",
    "            hits_mongo_ids.append(doc['mongo_id'])\n",
    "# Convert to set for fast parsing\n",
    "hits_mongo_ids = set(hits_mongo_ids)\n",
    "\n",
    "def check_for_hit(row, ads):\n",
    "    '''\n",
    "    Here we make a function that'll help us look at and modify the\n",
    "    dataframe to tell us if something is a hit\n",
    "\n",
    "    Inputs:\n",
    "        row     Pandas dataframe row\n",
    "        ads     String indicating the adsorbate you're looking at\n",
    "    Output:\n",
    "        Boolean indicating whether that row is a hit or not\n",
    "    '''\n",
    "    lower_bound, upper_bound = hit_ranges[ads]\n",
    "    if str(row['mongo_id']) in hits_mongo_ids:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T04:54:49.370361Z",
     "start_time": "2018-06-22T04:54:45.867097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAADXCAYAAAAQjIhsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b778cda6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAADXCAYAAAAQjIhsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b77939485d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot settings\n",
    "figsize = (7.08661, 3.0)   # 7.08661 is 180 mm in inches (required by Nature)\n",
    "#figsize = (3.46457, 3.0)   # 3.46457 is 88 mm in inches (required by Nature)\n",
    "ylim = (0, 1)\n",
    "yticks = [0., 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "sns.set_style('whitegrid')\n",
    "color = 'gray'\n",
    "\n",
    "# Make a plot for each adsorbate\n",
    "for ads in adsorbates:\n",
    "    df = dfs[ads]\n",
    "\n",
    "    # Use the calculation date to bucket everything\n",
    "    # into readable month/year strings\n",
    "    readable_dates = []\n",
    "    for time in df['adslab_calculation_date']:\n",
    "        month = time.strftime('%b')\n",
    "        year = str(time.year)\n",
    "        readable_dates.append(month + ' ' + year)\n",
    "    df['Date'] = readable_dates\n",
    "    # Initialize subplotting, since we're going to have both hits and\n",
    "    # error distributions vs. time\n",
    "    fig, axes = plt.subplots(2, figsize=figsize, sharex=True)\n",
    "    ax_hits, ax_dist = axes\n",
    "    # Push the subplots together\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.05)\n",
    "\n",
    "    # Convert errors into absolute errors\n",
    "    df['Absolute Error [eV]'] = np.abs(df['Error [eV]'])\n",
    "    # Create and format the distribution plot\n",
    "    ax_dist = sns.violinplot(x='Date', y='Absolute Error [eV]', data=df,\n",
    "                             ax=ax_dist, cut=0, scale='width')\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    ax_dist.set_xlabel('')\n",
    "    ax_dist.set_ylim((ylim))\n",
    "    ax_dist.set_yticks(yticks)\n",
    "    ax_dist.tick_params(labelright=True)\n",
    "    # Add dashed line\n",
    "    ax_dist.plot([-1, 13], [0.2, 0.2], 'k--', linewidth=1)\n",
    "\n",
    "    # Mark whether or not data is a \"hit\"\n",
    "    df['Hits'] = df.apply(lambda row: check_for_hit(row, ads), axis=1)\n",
    "    # Figure out the months/bins to plot by\n",
    "    months = [label.get_text() for label in ax_dist.get_xticklabels()]\n",
    "    # Calculate the counts within each bin\n",
    "    hits = [0]*len(months)\n",
    "    for m, month in enumerate(months):\n",
    "        for date, hit in zip(df['Date'], df['Hits']):\n",
    "            if hit and date == month:\n",
    "                hits[m] += 1\n",
    "    # Create the hits plot\n",
    "    ax_hits = sns.barplot(x=months, y=hits, ax=ax_hits)\n",
    "    # Label the actual number of hits on top of the bars\n",
    "    for tick, hit in zip(range(len(months)), hits):\n",
    "        ax_hits.text(x=tick, y=hit+0.5, s=hit, horizontalalignment='center', color='k')\n",
    "    # Misc formatting for the hits plot\n",
    "    ax_hits.set_yticks([])\n",
    "    low_ylim, high_ylim = ax_hits.get_ylim()\n",
    "    if ads == 'CO':\n",
    "        ax_hits.text(0, high_ylim-4, '# active surfaces identified')\n",
    "    elif ads == 'H':\n",
    "        ax_hits.text(0, high_ylim-4, '# active surfaces identified')\n",
    "    ax_hits.set_ylim((low_ylim, high_ylim+3))\n",
    "\n",
    "    # Save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('conv_for_%s.pdf' % ads, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For CO:\n",
      "    RMSE = 0.46\n",
      "    MAE = 0.29\n",
      "    MAD = 0.17\n",
      "For H:\n",
      "    RMSE = 0.41\n",
      "    MAE = 0.24\n",
      "    MAD = 0.16\n"
     ]
    }
   ],
   "source": [
    "# Pull out the error data for each adsorbate\n",
    "for ads in adsorbates:\n",
    "    df = dfs[ads]\n",
    "    errors = df['Error [eV]']\n",
    "    # Calculate metrics from the errors\n",
    "    y_true = np.zeros(len(errors))\n",
    "    y_pred = errors\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mad = metrics.median_absolute_error(y_true, y_pred)\n",
    "    # Report\n",
    "    print('For %s:' % ads)\n",
    "    print('    RMSE = %.2f' % rmse)\n",
    "    print('    MAE = %.2f' % mae)\n",
    "    print('    MAD = %.2f' % mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:GASpy_conda]",
   "language": "python",
   "name": "conda-env-GASpy_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
